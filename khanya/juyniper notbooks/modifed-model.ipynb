{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import joblib\n",
    "import re\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(r'C:\\Users\\honey\\Documents\\placment work\\Electronical-AI-Device-Recogniser\\khanya\\data managment\\datasets\\acs-f2-dataset.csv')\n",
    "\n",
    "# Data Preprocessing\n",
    "# Drop the 'time' column as it's not relevant for prediction\n",
    "df = df.drop('time', axis=1)\n",
    "\n",
    "# Convert categorical labels to numeric\n",
    "le = LabelEncoder()\n",
    "df['equipment'] = le.fit_transform(df['equipment'])\n",
    "\n",
    "# Split features and labels\n",
    "X = df.drop('equipment', axis=1)\n",
    "y = df['equipment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Handle class imbalance using SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# One-hot encode the target labels\n",
    "num_classes = len(np.unique(y))\n",
    "y_resampled_onehot = to_categorical(y_resampled, num_classes=num_classes)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled_onehot, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict appliance and probabilities\n",
    "def predict_appliance(electrical_readings):\n",
    "    # Ensure the input is a 2D array\n",
    "    if electrical_readings.ndim == 1:\n",
    "        electrical_readings = electrical_readings.reshape(1, -1)\n",
    "    \n",
    "    # Scale the input\n",
    "    scaled_input = scaler.transform(electrical_readings)\n",
    "    \n",
    "    # Make prediction\n",
    "    probabilities = model.predict(scaled_input)[0]\n",
    "    predicted_class = le.inverse_transform([np.argmax(probabilities)])[0]\n",
    "    \n",
    "    return predicted_class, probabilities\n",
    "\n",
    "# Example usage\n",
    "example_reading = np.array([50.0, 340, 111.284, -40.89, 0.533, 224.038])\n",
    "predicted_appliance, probabilities = predict_appliance(example_reading)\n",
    "\n",
    "print(f\"Predicted appliance: {predicted_appliance}\")\n",
    "print(\"Probabilities for each appliance:\")\n",
    "for appliance, prob in zip(le.classes_, probabilities):\n",
    "    print(f\"{appliance}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot probabilities\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=le.classes_, y=probabilities)\n",
    "plt.title('Appliance Prediction Probabilities')\n",
    "plt.xlabel('Appliance')\n",
    "plt.ylabel('Probability')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List and find versions of Keras model files in the directory\n",
    "found_files = []\n",
    "for file_name in os.listdir(directory_path):\n",
    "    match = pattern.match(file_name)\n",
    "    if match:\n",
    "        version = match.group(1)\n",
    "        found_files.append((file_name, version))\n",
    "\n",
    "if found_files:\n",
    "    for file_name, version in found_files:\n",
    "        print(f\"Found file: {file_name} with version: {version}\")\n",
    "else:\n",
    "    print(\"No Keras files found in the directory.\")\n",
    "\n",
    "def get_next_version(file_name):\n",
    "    # Find version pattern like 1.1.2\n",
    "    match = re.search(r'(\\d+\\.\\d+\\.\\d+)', file_name)\n",
    "    if match:\n",
    "        version_str = match.group(1)\n",
    "        version_parts = list(map(int, version_str.split('.')))\n",
    "        version_parts[-1] += 1  # Increment the last number\n",
    "        next_version = '.'.join(map(str, version_parts))\n",
    "        return file_name.replace(version_str, next_version)\n",
    "    else:\n",
    "        raise ValueError(\"No version found in file name\")\n",
    "\n",
    "# Example for scaler\n",
    "scaler_file = 'scaler1.1.2.pkl'\n",
    "next_scaler_file = get_next_version(scaler_file)\n",
    "joblib.dump(scaler, next_scaler_file)\n",
    "print(f\"Saved scaler as: {next_scaler_file}\")\n",
    "\n",
    "# Example for model\n",
    "model_file = 'appliance_recogniser#1.1.2.keras'\n",
    "next_model_file = get_next_version(model_file)\n",
    "model.save(next_model_file)\n",
    "print(f\"Saved model as: {next_model_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for model\n",
    "model_file = 'appliance_recogniser#1.1.2.keras'\n",
    "next_model_file = get_next_version(model_file)\n",
    "model.save(next_model_file)\n",
    "print(f\"Saved model as: {next_model_file}\")\n",
    "\n",
    "# Cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Prepare for storing metrics\n",
    "all_confusion_matrices = []\n",
    "all_classification_reports = []\n",
    "\n",
    "# Cross-validation loop\n",
    "for train_index, test_index in kf.split(X_resampled):\n",
    "    # Split the data\n",
    "    X_train, X_test = X_resampled[train_index], X_resampled[test_index]\n",
    "    y_train, y_test = y_resampled[train_index], y_resampled[test_index]\n",
    "    \n",
    "    # One-hot encode the labels\n",
    "    y_train_onehot = to_categorical(y_train, num_classes=num_classes)\n",
    "    y_test_onehot = to_categorical(y_test, num_classes=num_classes)\n",
    "    \n",
    "    # Build and compile the model\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=X_train.shape[1]),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train_onehot, epochs=50, verbose=0)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Store confusion matrix and classification report for this fold\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "    all_confusion_matrices.append(conf_matrix)\n",
    "\n",
    "    class_report = classification_report(y_test, y_pred_classes, output_dict=True)\n",
    "    all_classification_reports.append(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Ensure all_confusion_matrices is a list of numpy arrays\n",
    "# all_confusion_matrices = [np.array(cm) for cm in all_confusion_matrices if isinstance(cm, np.ndarray)]\n",
    "\n",
    "# # Filter out any non-array elements\n",
    "# all_confusion_matrices = [cm for cm in all_confusion_matrices if isinstance(cm, np.ndarray)]\n",
    "\n",
    "# # Ensure the list is not empty before calculating the mean\n",
    "# if all_confusion_matrices:\n",
    "#     average_conf_matrix = np.mean(all_confusion_matrices, axis=0)\n",
    "#     average_classification_report = {key: np.mean([report[key]['f1-score'] for report in all_classification_reports])\n",
    "#                                      for key in all_classification_reports[0].keys()}\n",
    "\n",
    "#     print('Average Confusion Matrix:')\n",
    "#     print(average_conf_matrix)\n",
    "\n",
    "#     print('Average Classification Report (F1-Scores):')\n",
    "# else:\n",
    "#     print('No valid confusion matrices to average.')\n",
    "# for key, score in average_classification_report.items():\n",
    "#     print(f'{key}: {score}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
