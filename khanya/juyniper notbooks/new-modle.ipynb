{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifying the provided 'edited-model.py' to incorporate suggested improvements\n",
    "\n",
    "# Modified script content with additional tuning, early stopping, and class distribution checks.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('C:/Users/honey/Documents/placment work/Electronical-AI-Device-Recogniser/khanya/data managment/datasets/acs-f2-dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before SMOTE:\n",
      "equipment\n",
      "3     10776\n",
      "2     10772\n",
      "12    10763\n",
      "13    10762\n",
      "14    10751\n",
      "1     10747\n",
      "11    10742\n",
      "0     10737\n",
      "4     10735\n",
      "6     10734\n",
      "5     10733\n",
      "10    10727\n",
      "8     10716\n",
      "9     10711\n",
      "7     10702\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "df = df.drop('time', axis=1)\n",
    "\n",
    "# Convert categorical labels to numeric\n",
    "le = LabelEncoder()\n",
    "df['equipment'] = le.fit_transform(df['equipment'])\n",
    "\n",
    "# Split features and labels\n",
    "X = df.drop('equipment', axis=1)\n",
    "y = df['equipment']\n",
    "\n",
    "# Analyze class distribution before SMOTE\n",
    "print(\"Class distribution before SMOTE:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Handle class imbalance using SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = sm.fit_resample(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after SMOTE:\n",
      "equipment\n",
      "0     10776\n",
      "1     10776\n",
      "2     10776\n",
      "3     10776\n",
      "4     10776\n",
      "5     10776\n",
      "6     10776\n",
      "7     10776\n",
      "8     10776\n",
      "9     10776\n",
      "10    10776\n",
      "11    10776\n",
      "12    10776\n",
      "13    10776\n",
      "14    10776\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Analyze class distribution after SMOTE\n",
    "print(\"Class distribution after SMOTE:\")\n",
    "print(pd.Series(y_resampled).value_counts())\n",
    "\n",
    "# One-hot encode the target labels\n",
    "num_classes = len(np.unique(y))\n",
    "y_resampled_onehot = to_categorical(y_resampled, num_classes=num_classes)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled_onehot, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m4041/4041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.3261 - loss: 1.9980 - val_accuracy: 0.5064 - val_loss: 1.4393\n",
      "Epoch 2/100\n",
      "\u001b[1m4041/4041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - accuracy: 0.4730 - loss: 1.4695 - val_accuracy: 0.5696 - val_loss: 1.2398\n",
      "Epoch 3/100\n",
      "\u001b[1m4041/4041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - accuracy: 0.5259 - loss: 1.3145 - val_accuracy: 0.6014 - val_loss: 1.1137\n",
      "Epoch 4/100\n",
      "\u001b[1m4041/4041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - accuracy: 0.5632 - loss: 1.2117 - val_accuracy: 0.6458 - val_loss: 1.0370\n",
      "Epoch 5/100\n",
      "\u001b[1m4041/4041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.5943 - loss: 1.1327 - val_accuracy: 0.6645 - val_loss: 0.9734\n",
      "Epoch 6/100\n",
      "\u001b[1m4041/4041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6138 - loss: 1.0827 - val_accuracy: 0.6787 - val_loss: 0.9304\n",
      "Epoch 7/100\n",
      "\u001b[1m4041/4041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.6286 - loss: 1.0421 - val_accuracy: 0.6854 - val_loss: 0.8983\n",
      "Epoch 8/100\n",
      "\u001b[1m4041/4041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.6381 - loss: 1.0090 - val_accuracy: 0.6928 - val_loss: 0.8694\n",
      "Epoch 9/100\n",
      "\u001b[1m2255/4041\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 921us/step - accuracy: 0.6495 - loss: 0.9818"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Concatenate, Multiply\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Define the model architecture with weighted input for 'freq' column and dropout\n",
    "input_layer = Input(shape=(X_train.shape[1],))\n",
    "freq_weight = K.constant([[2.0]])  # Convert the weight to a tensor with the same shape as freq_input\n",
    "\n",
    "# Split the input into 'freq' and other features\n",
    "freq_input = input_layer[:, 0:1]\n",
    "other_features = input_layer[:, 1:]\n",
    "\n",
    "# Apply the weight to the 'freq' column\n",
    "weighted_freq = Multiply()([freq_input, freq_weight])\n",
    "\n",
    "# Concatenate the weighted 'freq' column back with the other features\n",
    "weighted_input = Concatenate()([weighted_freq, other_features])\n",
    "\n",
    "# Define the rest of the model\n",
    "x = Dense(256, activation='relu')(weighted_input)\n",
    "x = Dropout(0.3)(x)  # Increased dropout for stronger regularization\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "output_layer = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model with a lower learning rate for better generalization\n",
    "model.compile(optimizer=Adam(learning_rate=0.0005), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(X_train, y_train, \n",
    "                    validation_data=(X_test, y_test),\n",
    "                    epochs=100, \n",
    "                    batch_size=32, \n",
    "                    callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LabelEncoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Define the class names\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m le \u001b[38;5;241m=\u001b[39m \u001b[43mLabelEncoder\u001b[49m()  \u001b[38;5;66;03m# Ensure le is defined\u001b[39;00m\n\u001b[0;32m      3\u001b[0m le\u001b[38;5;241m.\u001b[39mfit(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mequipment\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m# Fit the LabelEncoder with the 'equipment' column\u001b[39;00m\n\u001b[0;32m      4\u001b[0m class_names \u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39mclasses_\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LabelEncoder' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the class names\n",
    "le = LabelEncoder()  # Ensure le is defined\n",
    "le.fit(df['equipment'])  # Fit the LabelEncoder with the 'equipment' column\n",
    "class_names = le.classes_\n",
    "\n",
    "# Plot the confusion matrix with labels\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new-scaler1.0.2.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained model and the scaler for future use\n",
    "model.save('new-model1.0.3.keras')\n",
    "joblib.dump(scaler, 'new-scaler1.0.3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Return the new path so the user can download it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
