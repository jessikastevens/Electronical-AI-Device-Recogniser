{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(r'C:\\Users\\honey\\Documents\\placment work\\Electronical-AI-Device-Recogniser\\khanya\\data managment\\acs-f2-dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data Preprocessing\n",
    "# Drop the 'time' column as it's not relevant for prediction\n",
    "df = df.drop('time', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical labels to numeric\n",
    "le = LabelEncoder()\n",
    "df['equipment'] = le.fit_transform(df['equipment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and labels\n",
    "X = df.drop('equipment', axis=1)\n",
    "y = df['equipment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bda6478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle class imbalance using SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Apply SMOTE to create a balanced dataset\n",
    "sm = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = sm.fit_resample(X, y)\n",
    "\n",
    "# One-hot encode the target labels\n",
    "num_classes = len(np.unique(y))  # Number of output classes\n",
    "y_resampled_onehot = to_categorical(y_resampled, num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcfe0aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\honey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3631/3631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.1735 - loss: 3.3769 - val_accuracy: 0.1160 - val_loss: 6.7266\n",
      "Epoch 2/50\n",
      "\u001b[1m3631/3631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 982us/step - accuracy: 0.2636 - loss: 2.1042 - val_accuracy: 0.1067 - val_loss: 9.0241\n",
      "Epoch 3/50\n",
      "\u001b[1m3631/3631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 996us/step - accuracy: 0.2981 - loss: 2.0210 - val_accuracy: 0.1133 - val_loss: 10.2730\n",
      "Epoch 4/50\n",
      "\u001b[1m3631/3631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 992us/step - accuracy: 0.3551 - loss: 1.8897 - val_accuracy: 0.1289 - val_loss: 12.7819\n",
      "Epoch 5/50\n",
      "\u001b[1m3631/3631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 978us/step - accuracy: 0.3809 - loss: 1.8164 - val_accuracy: 0.1270 - val_loss: 13.8524\n",
      "Epoch 6/50\n",
      "\u001b[1m3631/3631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.3908 - loss: 1.7935 - val_accuracy: 0.1210 - val_loss: 16.6151\n",
      "Epoch 7/50\n",
      "\u001b[1m3631/3631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 990us/step - accuracy: 0.3982 - loss: 1.7709 - val_accuracy: 0.1304 - val_loss: 22.3982\n",
      "Epoch 8/50\n",
      "\u001b[1m3631/3631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 975us/step - accuracy: 0.4042 - loss: 1.7549 - val_accuracy: 0.1350 - val_loss: 22.9684\n",
      "Epoch 9/50\n",
      "\u001b[1m3631/3631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 994us/step - accuracy: 0.4120 - loss: 1.7446 - val_accuracy: 0.1265 - val_loss: 23.6475\n",
      "Epoch 10/50\n",
      "\u001b[1m3631/3631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 993us/step - accuracy: 0.4169 - loss: 1.7235 - val_accuracy: 0.1299 - val_loss: 27.9896\n",
      "Epoch 11/50\n",
      "\u001b[1m3631/3631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 976us/step - accuracy: 0.4200 - loss: 1.7163 - val_accuracy: 0.1258 - val_loss: 29.3233\n",
      "Epoch 12/50\n",
      "\u001b[1m3631/3631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.4225 - loss: 1.7100 - val_accuracy: 0.1306 - val_loss: 30.0647\n",
      "Epoch 13/50\n",
      "\u001b[1m3631/3631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 983us/step - accuracy: 0.4257 - loss: 1.6883 - val_accuracy: 0.1270 - val_loss: 33.4326\n",
      "Epoch 14/50\n",
      "\u001b[1m3631/3631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 969us/step - accuracy: 0.4229 - loss: 1.6488 - val_accuracy: 0.1336 - val_loss: 32.9836\n",
      "Epoch 15/50\n",
      "\u001b[1m3631/3631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 984us/step - accuracy: 0.4413 - loss: 1.5946 - val_accuracy: 0.1393 - val_loss: 39.9615\n",
      "Epoch 16/50\n",
      "\u001b[1m3631/3631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 971us/step - accuracy: 0.4577 - loss: 1.5563 - val_accuracy: 0.1407 - val_loss: 39.8355\n",
      "Epoch 17/50\n",
      "\u001b[1m3631/3631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 965us/step - accuracy: 0.4649 - loss: 1.5334 - val_accuracy: 0.1349 - val_loss: 35.0077\n",
      "Epoch 18/50\n",
      "\u001b[1m3631/3631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 983us/step - accuracy: 0.4731 - loss: 1.5099 - val_accuracy: 0.1744 - val_loss: 38.4299\n",
      "Epoch 19/50\n",
      "\u001b[1m3631/3631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 996us/step - accuracy: 0.4770 - loss: 1.4972 - val_accuracy: 0.1702 - val_loss: 39.0258\n",
      "Epoch 20/50\n",
      "\u001b[1m3631/3631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 987us/step - accuracy: 0.4830 - loss: 1.4723 - val_accuracy: 0.1885 - val_loss: 39.9827\n",
      "Epoch 21/50\n",
      "\u001b[1m3631/3631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 978us/step - accuracy: 0.4880 - loss: 1.4632 - val_accuracy: 0.1950 - val_loss: 39.5553\n",
      "Epoch 22/50\n",
      "\u001b[1m3631/3631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 973us/step - accuracy: 0.4926 - loss: 1.4493 - val_accuracy: 0.1827 - val_loss: 35.2294\n",
      "Epoch 23/50\n",
      "\u001b[1m3631/3631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 987us/step - accuracy: 0.4982 - loss: 1.4353 - val_accuracy: 0.1957 - val_loss: 36.1599\n",
      "Epoch 24/50\n",
      "\u001b[1m3631/3631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 987us/step - accuracy: 0.5016 - loss: 1.4280 - val_accuracy: 0.1927 - val_loss: 37.5321\n",
      "Epoch 25/50\n",
      "\u001b[1m3631/3631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 996us/step - accuracy: 0.5072 - loss: 1.4091 - val_accuracy: 0.1930 - val_loss: 38.1763\n",
      "Epoch 26/50\n",
      "\u001b[1m3631/3631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.5040 - loss: 1.4251 - val_accuracy: 0.2028 - val_loss: 37.8825\n",
      "Epoch 27/50\n",
      "\u001b[1m3631/3631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.5114 - loss: 1.3959 - val_accuracy: 0.2069 - val_loss: 34.5006\n",
      "Epoch 28/50\n",
      "\u001b[1m3631/3631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.5135 - loss: 1.3951 - val_accuracy: 0.1848 - val_loss: 37.5079\n",
      "Epoch 29/50\n",
      "\u001b[1m3631/3631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.5126 - loss: 1.4051 - val_accuracy: 0.1643 - val_loss: 30.4981\n",
      "Epoch 30/50\n",
      "\u001b[1m3631/3631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.5114 - loss: 1.4068 - val_accuracy: 0.1844 - val_loss: 37.6917\n",
      "Epoch 31/50\n",
      "\u001b[1m3631/3631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.5170 - loss: 1.3877 - val_accuracy: 0.1933 - val_loss: 33.5988\n",
      "Epoch 32/50\n",
      "\u001b[1m3631/3631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.5200 - loss: 1.3834 - val_accuracy: 0.2008 - val_loss: 36.0453\n",
      "Epoch 33/50\n",
      "\u001b[1m3631/3631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.5193 - loss: 1.3883 - val_accuracy: 0.2042 - val_loss: 35.4412\n",
      "Epoch 34/50\n",
      "\u001b[1m3631/3631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.5198 - loss: 1.3799 - val_accuracy: 0.2063 - val_loss: 37.0444\n",
      "Epoch 35/50\n",
      "\u001b[1m 678/3631\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 810us/step - accuracy: 0.5189 - loss: 1.3774"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build an improved neural network model with more layers and dropout for regularization\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(Dense(128, input_dim=X.shape[1], activation='relu'))\n",
    "\n",
    "# Hidden layers with dropout\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))  # Dropout for regularization\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.3))  # Dropout for regularization\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(len(np.unique(y)), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with validation\n",
    "history = model.fit(X_resampled, y_resampled_onehot, epochs=50, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d809ea87",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, confusion_matrix\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Predict on the validation set\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mX_test\u001b[49m)\n\u001b[0;32m      6\u001b[0m y_pred_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_pred, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Confusion matrix\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate model performance\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, y_pred_classes, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to one-hot encoding\n",
    "y_encoded = to_categorical(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(y_encoded.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=32, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict appliance and probabilities\n",
    "def predict_appliance(electrical_readings):\n",
    "    # Ensure the input is a 2D array\n",
    "    if electrical_readings.ndim == 1:\n",
    "        electrical_readings = electrical_readings.reshape(1, -1)\n",
    "    \n",
    "    # Scale the input\n",
    "    scaled_input = scaler.transform(electrical_readings)\n",
    "    \n",
    "    # Make prediction\n",
    "    probabilities = model.predict(scaled_input)[0]\n",
    "    predicted_class = le.inverse_transform([np.argmax(probabilities)])[0]\n",
    "    \n",
    "    return predicted_class, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "example_reading = np.array([50.0, 340, 111.284, -40.89, 0.533, 224.038])\n",
    "predicted_appliance, probabilities = predict_appliance(example_reading)\n",
    "\n",
    "print(f\"Predicted appliance: {predicted_appliance}\")\n",
    "print(\"Probabilities for each appliance:\")\n",
    "for appliance, prob in zip(le.classes_, probabilities):\n",
    "    print(f\"{appliance}: {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot probabilities\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=le.classes_, y=probabilities)\n",
    "plt.title('Appliance Prediction Probabilities')\n",
    "plt.xlabel('Appliance')\n",
    "plt.ylabel('Probability')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def get_next_version(file_name):\n",
    "    # Find version pattern like 1.1.2.3\n",
    "    match = re.search(r'(\\d+\\.\\d+\\.\\d+\\.\\d+)', file_name)\n",
    "    if match:\n",
    "        version_str = match.group(1)\n",
    "        version_parts = list(map(int, version_str.split('.')))\n",
    "        version_parts[-1] += 1  # Increment the last number\n",
    "        next_version = '.'.join(map(str, version_parts))\n",
    "        return file_name.replace(version_str, next_version)\n",
    "    else:\n",
    "        raise ValueError(\"No version found in file name\")\n",
    "\n",
    "# Example for scaler\n",
    "scaler_file = 'scaler1.1.2.3.pkl'\n",
    "next_scaler_file = get_next_version(scaler_file)\n",
    "joblib.dump(scaler, next_scaler_file)\n",
    "print(f\"Saved scaler as: {next_scaler_file}\")\n",
    "\n",
    "# Example for model\n",
    "model_file = 'appliance_recogniser#1.1.3.keras'\n",
    "next_model_file = get_next_version(model_file)\n",
    "model.save(next_model_file)\n",
    "print(f\"Saved model as: {next_model_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
